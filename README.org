#+TITLE: Optimisation Toolkit, /OPTK/

[[Tests][https://github.com/MaximeRobeyns/OPTK/workflows/Project%20build%20and%20test/badge.svg]] [[Documentation Build][https://github.com/MaximeRobeyns/OPTK/workflows/Compile%20and%20re-deploy%20documentation/badge.svg]]

*Warning*: This library is not yet finished. Please only expect useful output
when this warning message has been removed and replaced with a usage guide.

Developer documentation can be found at https://maximerobeyns.github.io/OPTK.

** What is OPTK?

OPTK (read /optic/) is a library for developing, benchmarking and comparing
*derivative-free* (`black box') optimisation algorithms, with an emphasis on
both developing novel algorithms and using them in real applications.

It is written in C/C++ to encourage and facilitate high-performance
implementations of optimisation algorithms, while at the same time allowing for
bindings to other languages such as Python, MATLAB or Octave to be easily
written. This means that existing implementations of algorithms in other
languages may be included in rankings, while allowing users to write new
algorithms in a language that they are comfortable with.

OPTK enforces a specification for new optimisation algorithms which is
designed to be compatible with the 'Tuner' interface from [[https://github.com/microsoft/nni][Microsoft's NNI
library]]. This means that algorithms developed for OPTK may be re-used directly
(i.e. without modification) with NNI's feature-rich web dashboard, CLI tools and
Kubernetes configuration for use in production.

** Concepts

*** What is an optimisation algorithm or 'tuner'?

These both refer to a piece of software which implements certain functions which
are called by either OPTK during benchmarks, or NNI during production runs. At
the simplest level, these functions allow it to accept a parameter search space,
and then generate new parameters for function evaluation, as well as accepting
the corresponding result.

*** What are the benchmarks?

OPTK provides C++ implementations of derivative-free global optimisation
benchmarks, notably the test functions in [[https://arxiv.org/abs/1308.4008][this literature survey]] (Jamil et
al. 2013), as well as implementations of the [[https://github.com/sigopt/evalset][evalset]] functions from Dewancker
et al. 2016.

I further plan to add the [[https://github.com/google-research/nasbench][neural architecture search benchmarks]] (Ying et al.
2019).

*** Algorithm Comparison

The underlying premise of this repository is to be able to rank and compare
optimisation algorithms, based on their performance on the same set of
benchmarks. Performance is measured by taking trajectories and calculating the
area under the curve. Further, we wish to measure which optimisation algorithms
perform well on functions with differing characteristics (e.g. functions which
are scalable, continuous, or separable etc.), so as to offer a practical
recommendation for practitioners wishing to select an optimisation algorithm for
their problem.


#+BEGIN_COMMENT
** Usage Guide

/Warning/ this is purely speculative

Here is the command line interface which is acting as a specification for what
the program does:

#+BEGIN_SRC bash
Usage: optk [options] algorithm...

Options:
  -b <benchmark>...   Only run the specified <benchmark>
  -o <dir>            Place the outputs into <dir>

Examples:
./optk gp_tuner
./optk -b synthetic nas -o /results/test1 smac
#+END_SRC

At the simplest level, =optk= works as a benchmarking program, which takes in an
optimisation algorithm, and will produce a set of traces (iteration, objective
value) pairs for each benchmark, which it will save in a csv file in the
=/results= directory in a file with the same name as the provided algorithm:

#+END_COMMENT

* Licence

Copyright (C) 2020 Maxime Robeyns

Written for the Advanced Computing Research Centre, University of Bristol.

Licensed under the Educational Community License, Version 2.0
(the "License"); you may not use this file except in compliance
with the License. You may obtain a copy of the License at

http://www.osedu.org/licenses/ECL-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

* References

Jamil, M., & Yang, X., A Literature Survey of Benchmark Functions For Global Optimization Problems, International Journal of Mathematical Modelling and Numerical Optimisation, 4(2), 150 (2013).  http://dx.doi.org/10.1504/IJMMNO.2013.055204

Dewancker, I., McCourt, M., Clark, S., Hayes, P., Johnson, A., & Ke, G., A Stratified Analysis of Bayesian Optimization Methods, arXiv:1603.09441 [cs, stat], (),  (2016).

Ying, C., Klein, A., Real, E., Christiansen, E., Murphy, K., & Hutter, F., NAS-Bench-101: Towards Reproducible Neural Architecture Search, arXiv:1902.09635 [cs, stat], (),  (2019).
